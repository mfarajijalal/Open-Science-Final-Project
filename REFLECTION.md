# Reflection – Open Science Final Project

This project was designed as a practical exercise in applying open science principles to a real-world research workflow. Rather than creating a synthetic or toy example, I chose to work with a small, de-identified subset of data derived from my ongoing research on vibration and acceleration measurements collected during tree-shaking experiments. This decision allowed me to engage directly with the real challenges of responsible data sharing, reproducibility, and transparency that researchers face in practice.

One of the most important lessons from this project was the distinction between openness and full disclosure. Although I possess a large collection of raw experimental data and advanced machine learning models developed for analysis, sharing the entire dataset publicly would raise ethical, privacy, and practical concerns. The full data include identifying metadata such as precise timestamps, experimental IDs, and location information, which should not be released openly. As a result, I curated a small, de-identified sample dataset that preserves the structure and scientific value of the measurements while removing sensitive details. This process reinforced the idea that open science is not about sharing everything indiscriminately, but about sharing responsibly.

Another key aspect of this project was reproducibility. To support this principle, I structured the repository in a clear and transparent way, separating raw data (data/) from analysis code (code/) and providing detailed documentation in the README files. Each CSV file corresponds to an individual tree and contains time-series sensor measurements with clearly defined columns. In addition, a minimal Python script was included to demonstrate how the data can be loaded, explored, and visualized. While this script does not represent the full machine learning pipeline used in my research, it serves as an accessible entry point for others who may wish to understand or reuse the dataset.

This design choice highlights another important insight from the course: open science artifacts should be usable by others, not only by the original author. Complex machine learning models often require extensive dependencies, computational resources, and domain-specific knowledge. By including a lightweight and readable example script, I aimed to lower the barrier to reuse while still maintaining scientific integrity. Advanced analyses can be described conceptually without necessarily being fully executable by every user.

Documentation also played a central role in this project. Writing the main README forced me to think carefully about how to explain the dataset, experimental context, and repository structure to someone unfamiliar with my research. This exercise revealed gaps that often exist between what researchers assume is “obvious” and what external users actually need to know. Clear documentation is not merely an administrative task; it is an essential component of reproducible and trustworthy science.

Finally, this project influenced how I think about my broader research practices. I now see open science not as an additional burden, but as a framework that improves research quality. The act of organizing data, simplifying code, and articulating assumptions makes the research process itself more robust. Moving forward, I plan to adopt these principles more systematically, including early planning for data sharing, clearer metadata standards, and better separation between exploratory and shareable research artifacts.

In summary, this project provided hands-on experience with the practical, ethical, and technical dimensions of open science. By sharing a real but carefully curated dataset alongside transparent documentation and example code, I demonstrated how meaningful openness can be achieved without compromising responsibility. The lessons learned here will directly inform my future research and contribute to more reproducible and accessible scientific work.
