This project was designed as a practical and reflective exercise in applying open science principles to a real-world research workflow. Rather than constructing a synthetic or toy example, I chose to work with a small, carefully curated, and de-identified subset of data derived from my ongoing PhD research on vibration and acceleration measurements collected during tree-shaking experiments. This choice allowed me to engage directly with the authentic challenges researchers face when attempting to balance transparency, reproducibility, and ethical responsibility in data sharing.

One of the most significant lessons learned from this project was the distinction between openness and full disclosure. While my broader research includes a large volume of raw experimental data and advanced machine learning models, releasing the full dataset publicly would introduce ethical, privacy, and practical concerns. The complete data contain identifying metadata such as precise timestamps, experimental identifiers, and location-related information that should not be openly shared. As a result, I developed a de-identified sample dataset that preserves the scientific structure and analytical value of the measurements while removing sensitive details. This process reinforced the understanding that open science is not about sharing all data indiscriminately, but about making deliberate, responsible decisions regarding what can and should be shared.

Reproducibility was another central focus of this project. To support reproducible research practices, I organized the repository using a clear and logical structure that separates raw data (data/) from analysis and visualization code (code/). Comprehensive documentation was provided through README files to explain the dataset, repository layout, and intended usage. Each CSV file corresponds to an individual tree and contains time-series sensor measurements with clearly defined columns and units. In addition, a minimal Python script was included to demonstrate how the data can be loaded, explored, and visualized. Although this script does not represent the full machine learning pipeline used in my research, it provides a transparent and accessible example that others can readily understand and reuse.

This design decision reflects an important insight emphasized throughout the course: open science artifacts should be usable by others, not only by the original author. Complex machine learning workflows often require extensive computational resources, specialized dependencies, and domain-specific expertise. By including a lightweight and readable example script, I aimed to lower the barrier to reuse while maintaining scientific rigor. Advanced analyses can be described conceptually and documented clearly without requiring every shared artifact to be fully executable by all potential users.

Documentation played a critical role in this process. Writing the main README required me to reconsider how to communicate experimental context, data structure, and assumptions to an audience unfamiliar with my research domain. This exercise revealed gaps between what researchers often assume to be self-explanatory and what external users actually need in order to interpret and reuse shared materials. I came to view documentation not as an administrative obligation, but as a foundational component of trustworthy and reproducible science.

This project also influenced how I approach my broader research practices. I now see open science not as an additional burden, but as a framework that strengthens research quality. The act of organizing data, simplifying code, and explicitly articulating assumptions improves clarity, reduces ambiguity, and increases long-term usability. Moving forward, I plan to integrate these principles earlier in my research workflow, including more systematic data management planning, clearer metadata standards, and a stronger separation between exploratory analyses and shareable research artifacts.

Beyond this individual project, these practices contribute to a broader culture of openness within the research community. By providing clear, ethical, and reusable scientific artifacts, researchers enable others—students, collaborators, and practitioners—to learn from, build upon, and critically evaluate shared work. Such practices are essential for advancing reproducibility, transparency, and collective scientific progress.

In summary, this project provided hands-on experience with the practical, ethical, and technical dimensions of open science. By sharing a real but responsibly curated dataset alongside transparent documentation and example code, I demonstrated how meaningful openness can be achieved without compromising ethical responsibility. The lessons learned through this process will directly inform my future research and support the development of more reproducible, accessible, and trustworthy scientific work.
